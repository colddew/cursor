# 阳光高考网招生章程数据爬取项目 PRD

> **项目状态**: ✅ 已完成
> **完成时间**: 2026-01-14
> **最终交付物**: `招生章程.xlsx`（2915所学校的完整信息，含招生章程详情页链接）
>
> **说明**: 本文档记录了阳光高考网数据爬取项目的完整开发过程、技术方案和经验总结。虽然项目已完成，但本文档保留的开发经验对于后续类似的数据处理项目具有重要的参考价值。

## 项目概述
从阳光高考网（gaokao.chsi.com.cn）爬取所有高校的招生章程数据，生成Excel文件。

---

## 核心技术要求

### 工具选择（关键约束）
- **仅使用 Playwright MCP**（`mcp__playwright__*` 系列工具）
- **严格禁止 Chrome DevTools MCP**（会被反爬虫识别，导致数据不完整）

### 数据字段（9个必填字段）
| 字段 | 说明 | 验证规则 |
|------|------|----------|
| 学校名称 | 高校全称 | 不能为空 |
| 省份 | 所属省份 | 不能为空 |
| 城市 | 直辖市=省份，其他可为空 | - |
| 主管部门 | 教育部/省教育厅等 | 不能为空 |
| 办学层次 | 本科 或 高职(专科) | 必须是二者之一 |
| 院校特性 | "双一流"/民办/独立学院等 | 可为空 |
| 校徽 | 图片地址 | 不能为空，格式：`https://t1.chei.com.cn/common/xh/xxxxx.jpg` |
| 招生章程链接 | 点击"招生章程>"的链接 | 不能为空 |
| 学校详情页链接 | 点击学校名称的链接 | 不能为空 |

---

## 爬取流程

### 第一页：初始加载
1. 打开第一页URL
2. 等待页面完全加载（网络空闲 + 图片加载完成）
3. 确认校徽图片数量充足（>90张）
4. 提取数据
5. 验证数据
6. 追加写入Excel

### 后续页：翻页爬取
1. **填写页码翻页**（推荐方式）
   - 直接在页码输入框中填写目标页码
   - 按Enter键或点击"跳转"按钮进行翻页
   - **不要使用"下一页"按钮**：实际爬取中发现点击"下一页"按钮多次导致翻页失败
   - 直接填写页码更可靠，能确保准确跳转到目标页面
2. 等待条件比第一页更严格（图片懒加载需要更长时间）
3. 快照确认页面状态
4. 提取数据
5. **立即验证数据**（发现异常立即停止）
6. 追加写入Excel

### 等待策略（经验教训）
**关键问题**: 后续页面的校徽变成 `default.jpg`

**原因分析**:
- 第一页：用户主动打开，浏览器触发图片加载，校徽正确
- 后续页：点击翻页后，图片可能还在视口外，懒加载未完成，src仍是默认值

**解决方案**:
- 提取前确认图片已加载（检查图片数量和src是否包含default.jpg）
- 使用动态等待而非固定时间（等待条件满足而非等待N秒）
- 后续页等待时间应比第一页更长（3-5秒）

---

## 数据验证

### 验证时机
**每页提取后立即验证，发现问题立即停止爬取**

不要等到最后才发现问题，这样会导致大量重复工作。

### 验证清单
每页数据必须通过以下检查：

1. **数据条数检查**
   - 前29页：每页100条
   - 第30页：15条
   - 发现条数不对立即停止

2. **必填字段检查**
   - 学校名称不能为空
   - 主管部门不能为空
   - 办学层次必须是"本科"或"高职(专科)"
   - 所有链接格式必须正确（以https://开头）

3. **校徽检查（关键指标）**
   - default.jpg 的占比应该 < 10%
   - 如果占比过高（如超过30%），说明图片未加载完成
   - 此时应增加等待时间或检查网络连接

4. **数据一致性检查**
   - 当前页的URL与期望页码一致
   - 不是重复上一页的数据

### 验证失败的处理
当验证失败时：
1. 立即停止爬取
2. 输出详细的错误信息
3. 给出具体的解决建议（如增加等待时间、检查网络等）
4. 不要跳过错误继续爬取

---

## 状态管理

### Compaction后的状态恢复
**问题**: compaction后会丢失浏览器会话信息，导致打开空白浏览器循环

**预防措施**:
1. 每次操作前检查现有标签页（使用browser_tabs）
2. 使用快照确认页面状态（使用browser_snapshot）
3. 如果发现页面异常，重新导航到正确URL

### 每次操作前的状态检查
- 确认浏览器会话正常
- 确认在正确页面
- 确认"下一页"按钮存在（未到最后一页）

---

## 性能优化

### 性能问题分析
| 问题 | 耗时占比 | 说明 |
|------|---------|------|
| 工具选择错误 | 30% | 使用Chrome DevTools导致失败和重试 |
| 重复爬取 | 20% | 数据错误需要重新爬取 |
| 过度延迟 | 20% | 固定等待时间过长 |
| 上下文恢复 | 15% | compaction后重新读取状态 |
| 低效提取 | 15% | JavaScript执行不够优化 |

### 优化策略

**1. 动态等待而非固定延迟**
- 等待特定条件满足（如图片加载完成）
- 而非固定等待N秒
- 这样可以减少等待时间，同时确保数据完整

**2. 增量写入而非批量写入**
- 每页提取后立即追加到Excel
- 不要在内存中积累所有数据
- 避免内存占用过大和上下文膨胀

**3. 中间数据存文件**
- 临时数据保存到 /tmp 目录
- 不要加载到对话上下文
- 减少compaction频率

**4. 只验证当前页**
- 只验证新增的数据
- 不要每次都读取整个Excel文件
- 减少I/O操作和上下文累积

---

## 反爬虫对策

### 1. 随机延迟
每页之间设置2-5秒的随机延迟，模拟人类操作。

### 2. 模拟人类操作
- 使用点击按钮而非直接修改URL
- 偶尔滚动页面
- 避免过于规律的操作模式

### 3. 错误重试机制
- 遇到错误等待后重试
- 最多重试3次
- 超过次数则停止并报告

---

## 常见问题和解决方案

### 问题1：校徽全部变成default.jpg
**原因**: 图片懒加载未完成，提取时img.src还是默认值

**解决方案**:
- 增加等待时间（3-5秒）
- 在提取前检查图片数量和src状态
- 使用waitForFunction等待图片加载完成

### 问题2：打开空白浏览器循环
**原因**: compaction后丢失浏览器会话，重复打开新标签页

**解决方案**:
- 每次操作前使用browser_tabs检查现有标签
- 使用browser_snapshot确认页面状态
- 不要假设浏览器状态

### 问题3：数据条数不对
**原因**: 页面未完全加载或提取逻辑有缺陷

**解决方案**:
- 增加等待时间
- 检查页面结构是否变化
- 验证提取逻辑

### 问题4：点击"下一页"按钮导致翻页失败
**原因**: 点击"下一页"按钮不可靠，多次爬取中出现翻页错误

**解决方案**:
- 直接在页码输入框中填写目标页码
- 按Enter键或点击"跳转"按钮进行翻页
- 避免使用"下一页"按钮，优先使用直接页码输入

### 问题5：频繁compaction导致上下文丢失
**原因**: 对话历史和中间数据累积过多

**解决方案**:
- 中间数据保存到文件，不放在上下文
- 减少大文件的重复读取
- 每页只验证新增数据，不重新验证全部

---

## 输出文件

| 文件名 | 说明 |
|--------|------|
| 招生章程.xlsx | 最终数据（2915所学校的完整信息） |
| 2025全国普通高等学校名单.xlsx | 教育部官方名单（参考数据源） |

### 数据处理脚本

| 文件名 | 说明 |
|--------|------|
| crawl_gaokao_schools.py | 阳光高考网爬虫，获取学校基础信息（2915所） |
| crawl_enrollment_details.py | 增量爬取招生章程详情页链接（分批次、断点续传） |
| fill_features_by_tags.py | 基于筛选标签补充院校特性数据（820所） |
| fill_city_data.py | 从教育部名单匹配城市数据（2652所） |
| fill_47_cities.py | 补充47所匹配失败院校的城市信息 |
| add_remarks.py | 添加备注列，标注匹配失败原因 |

### 脚本使用顺序

1. **crawl_gaokao_schools.py** - 爬取学校基础信息
2. **fill_features_by_tags.py** - 补充院校特性
3. **fill_city_data.py** - 匹配城市数据
4. **fill_47_cities.py** - 补充剩余城市
5. **add_remarks.py** - 添加备注
6. **crawl_enrollment_details.py** - 爬取招生章程详情页链接（如需要）

---

## 开发检查清单

### 爬取前
- 确认使用 Playwright MCP（不用 Chrome DevTools）
- 确认浏览器会话正常
- 确认网络连接正常

### 爬取中（每页）
- 数据条数正确（前29页100条，第30页15条）
- 学校名称全部非空
- 校徽中 default.jpg 占比 < 10%
- 所有链接格式正确
- 数据已追加写入Excel

### 爬取后
- Excel文件行数 = 预期学校数（2915）
- 双一流学校数量 ≈ 147所（对比参考名单）
- 抽查10条数据，确认字段完整

---

## 核心经验总结

1. **数据质量 > 速度**: 先确保正确，再优化性能
2. **状态检查**: 每次操作前检查当前状态，不假设
3. **增量验证**: 只验证新增数据，不重新验证全部
4. **及时发现问题**: 每页立即验证，不要等到最后
5. **动态等待**: 等待条件满足而非固定时间

---

## 招生章程详情页增量爬取经验

### 增量更新策略

**问题**: 分批次爬取时，如果等所有批次完成再一次性更新Excel，容易产生行号错位导致数据错乱。

**核心原则**: 每批次（15-20所）完成后立即根据学校名称精确匹配更新Excel，不要用行号。

**为什么用学校名称**:
- 行号会随删除、插入操作变化
- 学校名称是稳定的唯一标识
- 支持断点续传，不受中间批次影响

### 多文件同步验证

**同步文件**: Excel（主数据）+ CSV（队列状态）+ JSON（进度跟踪）

**验证方法**:
- 定期比对三个文件的数据一致性
- 发现数量不对立即停止分析
- 优先以Excel为准（用户查看的主文件）

**案例**: Excel显示85所无数据，JSON只记录65所失败，说明21所被错误标记为"完成"但实际无数据。解决方法是以Excel实际数据为准重新爬取。

### 无效数据标记

区分"未爬取"和"爬取但无数据"：
- `0-链接无效`: 链接彻底打不开
- `0-暂无2025年招生章程`: 页面明确提示无数据
- Excel中用灰色背景标识问题数据

### 反爬虫优化

**延迟策略**:
- 翻页后: 2.5-4秒（随机）
- 学校之间: 3-5.5秒（随机）
- 批次之间: 5-10秒（随机）
- 出错重试: 10-20秒（随机）

**行为模拟**:
- 30%概率随机滚动页面
- Excel更新后的自然间隔本身就是延迟

---

## 数据验证与更新经验（2026-01-14）

### 验证流程

**验证目标**: 对比原始数据和重新爬取的数据，找出不一致的记录。

**验证规模**: 2,915所学校，批量验证（20条/批）

**验证结果**:
- ✅ 验证通过: 2,353条 (80.7%)
- 🔄 待更新: 562条 (19.2%)
- ⚠️ 未找到详情页: 3条 (0.1%)

### 关键技术配置

**成功反爬虫的关键配置**:

```python
async with async_playwright() as p:
    browser = await p.chromium.launch(
        headless=False,
        args=['--disable-blink-features=AutomationControlled']  # 禁用自动化检测
    )

    context = await browser.new_context(
        user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        viewport={'width': 1920, 'height': 1080},
        locale='zh-CN',                    # 设置中文
        timezone_id='Asia/Shanghai',       # 设置中国时区
    )

    page = await context.new_page()

    await page.goto(url, timeout=30000, wait_until='domcontentloaded')
    await asyncio.sleep(2)  # 第一条等待2秒

    detail_links = await page.query_selector_all('a[href*="/zsgs/zhangcheng/listVerifedZszc--"]')
```

**失败案例**（缺少关键配置）:
```python
# ❌ 错误示例：缺少反爬虫配置，会被识别为机器人
browser = await p.chromium.launch(headless=True)
context = await browser.new_context()  # 没有任何配置参数
```

**对比分析**:

| 配置项 | 成功代码 | 失败代码 | 影响 |
|--------|---------|---------|------|
| 反自动化参数 | `--disable-blink-features=AutomationControlled` | ❌ 无 | 被识别为机器人 |
| User-Agent | 完整的浏览器标识 | ❌ 默认值 | 被识别为脚本 |
| 语言设置 | `locale='zh-CN'` | ❌ 无 | 语言不匹配 |
| 时区设置 | `timezone_id='Asia/Shanghai'` | ❌ 无 | 地理位置异常 |
| 视口大小 | 1920x1080 | ❌ 默认值 | 浏览器指纹异常 |
| headless | False | True | 无头模式易被检测 |

### 多条规章处理

**发现问题**: 部分学校有多个招生章程（如浙江工商大学有"普通高校"和"人民武装学院"两个规章）。

**存储格式**: 多条规章用换行符拼接
```
浙江工商大学人民武装学院2025年普通高校招生章程,https://...
浙江工商大学2025年普通高校招生章程,https://...
```

**标识字段**: 需要同时更新"多链接标识"列
```
是(2个) ⚠️需验证
```

**处理策略**:
- 抓取时获取所有规章，不只抓第一个
- 用换行符拼接多个规章
- 更新"多链接标识"字段

### 数据比对逻辑

**比对维度**:
1. **URL比对**: 优先比较完整URL（非infoId）
   - URL相同 → 标记"待更新"（标题格式差异）
   - URL不同 → 标记"不一致"（需进一步分析）

2. **标题比对**: 检查学校名称一致性
   - 完全一致: 正常
   - 标题包含学校名: 正常（如多了年份、"2025年招生章程"）
   - 差异较大: 需人工核对

**常见差异类型**:
- 括号格式: `()` vs `（）`（半角vs全角）
- 名称简化: 去掉"民办"前缀
- 标题简化: "2025年学校招生章程"
- 暂无数据: "暂无2025年招生章程"

### 更新原则

**核心原则**: 精确匹配更新，不用行号

```python
# ✅ 正确：根据学校名称匹配
mask = df_original['学校名称'] == school_name
if mask.sum() == 1:
    idx = df_original[mask].index[0]
    df.at[idx, '招生章程详情页链接'] = new_data

# ❌ 错误：使用行号（容易错位）
df.at[idx, '招生章程详情页链接'] = new_data
```

**更新字段**:
1. `招生章程详情页链接`: 新爬取的数据
2. `多链接标识`: 如果有多条规章

### 经验教训

1. **复用成功代码，不要重写简化版**
   - 已经验证成功的代码模式（如 verify_with_playwright.py）包含所有必要的反爬虫配置
   - 每次重新写简化版会遗漏关键配置，导致被反爬虫拦截
   - 不要假设"看起来差不多"就行

2. **多条规章问题**
   - 部分学校有多个招生章程，不能只抓第一个
   - 需要检查并抓取所有规章
   - 用换行符拼接，更新"多链接标识"

3. **数据验证的重要性**
   - 原始数据可能过时（infoId变化导致链接失效）
   - 需要定期重新爬取验证
   - 比对URL而非infoId（URL包含完整信息）

4. **更新时保持数据完整性**
   - 精确匹配更新，避免错位
   - 同时更新相关字段（多链接标识）
   - 不要修改未经验证的数据

---

## 项目经验总结（2026-01-15）

### 一、技术选型决策

**本地 Playwright vs MCP 工具**

| 对比维度 | 本地 Playwright | MCP 工具 |
|---------|----------------|---------|
| 成本 | 不消耗 Token | 消耗 Token |
| 适用场景 | 批量处理、自动化任务 | 交互式操作、一次性任务 |
| 调试便利性 | 可直接调试打印 | 需要通过对话查看结果 |
| 断点续传 | 容易实现（JSON 进度文件） | 难以实现 |
| 推荐场景 | 2915条全量验证 | 单条数据测试 |

**结论**：批量验证优先使用本地脚本，交互式探索使用 MCP 工具。

---

### 二、阳光高考网反爬虫机制与对策

**网站特征**：
- 返回 HTTP 412 错误（Precondition Failed）
- 检测浏览器自动化特征
- 检测 User-Agent、语言、时区

**必须包含的配置**（缺一不可）：

```python
# 1. 禁用自动化检测（核心）
args=['--disable-blink-features=AutomationControlled']

# 2. 完整浏览器指纹
user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
viewport={'width': 1920, 'height': 1080}
locale='zh-CN'
timezone_id='Asia/Shanghai'

# 3. 非无头模式
headless=False  # 无头模式更容易被检测
```

**延迟策略**：
- 第一条：2秒（确保页面完全加载）
- 后续：0.3-0.5秒（避免请求过快）
- 批次间：1.5-2秒（包含 Excel 写入时间）

**常见错误**：
- ❌ 简化配置（如省略 locale 或 timezone_id）
- ❌ 使用 headless=True
- ❌ 缩短延迟时间追求速度

---

### 三、数据架构设计

**单列 vs 多列**：

| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| 单列（用换行符拼接） | 节省列数 | Excel 中不便点击、难以程序化处理 | 数据采集阶段 |
| 4列拆分（名称+URL 各2列） | Excel 中可直接点击、易于程序处理 | 占用更多列 | 数据交付和维护阶段 |

**拆分逻辑**：
```
本部列：存储主招生章程（或 infoId 较大的，表示最新版本）
特殊列：存储分校、特殊类型招生章程（如人民武装学院）
```

**决策依据**：
- 如果只有 1 个章程 → 存入本部列，特殊列为空
- 如果有 2 个章程 → 按 infoId 大小排序，大的存本部，小的存特殊
- 如果有 3+ 个章程 → 需要人工判断，考虑扩展列数

---

### 四、批量处理最佳实践

**批次大小选择**：

| 批次大小 | 优点 | 缺点 | 推荐场景 |
|---------|------|------|----------|
| 10条/批 | 失败影响范围小 | 总耗时较长 | 不稳定网络环境 |
| 20条/批 | 平衡速度与稳定性 | - | **推荐默认值** |
| 50条/批 | 速度较快 | 失败后重试成本高 | 稳定网络 + 验证过的脚本 |

**断点续传设计**：

```python
# 进度文件结构（JSON）
{
  "start_time": "2026-01-14T10:00:00",
  "verified_count": 600,
  "current_batch": 30,
  "total_batches": 146
}

# 恢复逻辑
if progress['verified_count'] > 0:
    start_batch = progress['current_batch']
    # 跳过已完成的批次
```

**实时保存策略**：
- ❌ 错误：内存中积累所有数据，最后一次性保存
  - 风险：中断时全部数据丢失
- ✅ 正确：每批次完成后立即更新 Excel
  - 优势：中断后只损失当前批次

**更新时使用学校名称匹配，不用行号**：
```python
# ✅ 正确：精确匹配
mask = df['学校名称'] == school_name
idx = df[mask].index[0]

# ❌ 错误：使用行号
df.at[row_number, '列名'] = value  # 行号可能错位
```

---

### 五、错误处理策略

**可重试的错误**：
- 网络超时（timeout）
- 连接错误（connection refused）
- 412 错误（反爬虫触发）

**不可重试的错误**：
- 数据格式错误
- 逻辑错误
- 持续的 404 错误（页面已删除）

**多链接处理原则**：
1. 脚本自动检测多条链接
2. 记录到专门的清单文件
3. 标注"待人工判断"
4. 不要自动合并或选择

---

### 六、文件管理原则

**版本控制**：
- ✅ 立即提交：经过测试的工作脚本
- ❌ 不要提交：临时文件、日志、进度文件

**临时文件清理**：
- 清理前必须：列出文件、说明用途、用户确认
- 禁止删除：*.py 工作脚本
- 可以删除：*.log, *_progress.json, 测试文件

---

## 表格格式招生章程爬取经验（2026-01-15）

**项目背景**：800所院校（27.5%）的招生章程包含表格，页面结构复杂（纯表格/文字+表格/多次混合）。

**核心经验**：
1. **简单逻辑优于复杂实现**：字符串 substring 比节点遍历更可靠，直接操作 innerHTML 比复杂 DOM API 更易维护
2. **表格属性清理**：移除所有 style/class/width 等冗余属性，只保留 colspan/rowspan，可减少93%文件体积
3. **延迟策略优化**：第一页2秒确保加载，后续0.5秒快速切换，学校之间0.3-0.5秒，800所学校30分钟完成
4. **测试验证流程**：单学校测试 → 多结构测试 → 20所小批量 → 全量爬取，确保100%成功率